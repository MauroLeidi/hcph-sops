{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c558e9",
   "metadata": {},
   "source": [
    "# QA/QC of eye-tracking data\n",
    "\n",
    "This notebook visualizes ET data for the purpose of QC'ing the BIDS conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import json\n",
    "import ppjson\n",
    "from importlib import reload  # For debugging purposes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import eyetrackingrun as et\n",
    "from matplotlib import pyplot as plt\n",
    "import plot\n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3f8fb",
   "metadata": {},
   "source": [
    "In the schedule.tsv file, we've listed the EDF files we created and their associated sessions. Now, let's check out what's in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d67d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_lookup = pd.read_csv(\"schedule.tsv\", sep=\"\\t\", na_values=\"n/a\")\n",
    "edf_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75306e",
   "metadata": {},
   "source": [
    "As an illustrative example, we'll handle the data from session 4. Replace `DATA_PATH` with your data's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/data/datasets/hcph-pilot-sourcedata/recordings/psychopy/\")\n",
    "session = 4\n",
    "et_session = edf_lookup[edf_lookup.session == session]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fcb08",
   "metadata": {},
   "source": [
    "# Eye-tracking during the diffusion weighted imaging (DWI) run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e51b5",
   "metadata": {},
   "source": [
    "Let's begin with the DWI run of the session selected above.\n",
    "We first create a new `EyeTrackingRun` object, encapsulating eye-tracking information in BIDS-like format.\n",
    "The corresponding *Psychopy* experiment sends the message `hello fixation` and `bye fixation` when the DWI run starts and stops, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_dwi = et.EyeTrackingRun.from_edf(\n",
    "    DATA_PATH / et_session.fixation_edf.values[0],\n",
    "    message_first_trigger=\"hello\",\n",
    "    message_last_trigger=\"bye\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e095e",
   "metadata": {},
   "source": [
    "The `et_dwi` object now has three relevant members: metadata, events and recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dac159",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    json.dumps(et_dwi.metadata, sort_keys=True, indent=2, cls=ppjson.CompactJSONEncoder)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_dwi.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89380b3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "et_dwi.recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = et_dwi.recording[[\"eye1_x_coordinate\", \"eye1_y_coordinate\"]]\n",
    "\n",
    "data[data.eye1_x_coordinate.notna() & data.eye1_y_coordinate.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b7b44",
   "metadata": {},
   "source": [
    "### Plotting some data\n",
    "\n",
    "Let's first generate a time axis `t_axis` in seconds.\n",
    "To do so, we read the \"timestamp\" column of the dataframe, and divide by the sampling frequency (stored within the metadata).\n",
    "In order to make it more \"BIDS-y\", we also set the start of the run at zero by applying the start time metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee367657",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_axis = (\n",
    "    et_dwi.recording.timestamp.values - et_dwi.metadata[\"StartTime\"]\n",
    ") / et_dwi.metadata[\"SamplingFrequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e330b",
   "metadata": {},
   "source": [
    "Once we have the sampling axis, we can look at a basic measurement: pupil area in arbitrary units (as it comes from the EyeLink tracker).\n",
    "With our conversion into BIDS, pupil area (column `pa_right` of the EDF file) is mapped to the `eye1_pupil_size` column of the `_eyetrack.tsv.gz` file.\n",
    "We first create a figure with landscape ratio to better get a sense of the data along time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_dwi.recording[\"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"pupil area [a.u.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b42dd",
   "metadata": {},
   "source": [
    "We can zoom in into the early moments of the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_dwi.recording[\"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"pupil area [a.u.]\")\n",
    "plt.xlim((-1.0, 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0acea23",
   "metadata": {},
   "source": [
    "Next, we look at eye blinks.\n",
    "The two discontinuities at almost seconds 8 and 9 of the pupil area plot could be related to blinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_dwi.recording[\"eye1_blink\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"eyes closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a267ff",
   "metadata": {},
   "source": [
    "Looks like the eyes (or at least the right eye, which was tracked) were closed during most of the run.\n",
    "Let's now plot together the first ten seconds of pupil area AND the blinks binary signal.\n",
    "Indeed, the pupil area drops when blinks are happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_dwi.recording[\"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_dwi.recording[\"eye1_blink\"].values * 10000,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"pupil area [a.u.]\")\n",
    "plt.xlim((-0.1, 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533a9c1",
   "metadata": {},
   "source": [
    "We can clean up the pupil area time series by removing data while the eye was closed.\n",
    "The signal seems to display less artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pupil_area = et_dwi.recording[\"eye1_pupil_size\"].values\n",
    "pupil_area[et_dwi.recording[\"eye1_blink\"] > 0] = np.nan\n",
    "\n",
    "fig = plt.figure(figsize=(16, 2))\n",
    "\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_dwi.recording[\"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"pupil area [a.u.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db418a7a",
   "metadata": {},
   "source": [
    "We can also see the map of time spent on areas of the screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (\n",
    "    et_dwi.metadata[\"ScreenAOIDefinition\"][1][1],\n",
    "    et_dwi.metadata[\"ScreenAOIDefinition\"][1][3],\n",
    ")\n",
    "data = et_dwi.recording[[\"eye1_x_coordinate\", \"eye1_y_coordinate\"]]\n",
    "data = data[et_dwi.recording.eye1_blink < 1]\n",
    "plot.plot_heatmap_coordinate(data, density=False, screen_size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3fa78",
   "metadata": {},
   "source": [
    "At the start and end of the DWI run, we presented two central fixation points during 60s, serving as reference markers for the analysis of potential gaze drift (i.e., shifts in gaze coordinates). Let's compare the density maps during these two fixation points to assess the drift during the DWI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe892ec5",
   "metadata": {},
   "source": [
    "First, we extract the data corresponding to the two fixation points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixation_duration = 60\n",
    "\n",
    "start_first_fixation = et_dwi.metadata[\"StartTime\"]\n",
    "stop_first_fixation = start_first_fixation + (\n",
    "    fixation_duration * et_dwi.metadata[\"SamplingFrequency\"]\n",
    ")\n",
    "stop_second_fixation = et_dwi.metadata[\"StopTime\"]\n",
    "start_second_fixation = stop_second_fixation - (\n",
    "    fixation_duration * et_dwi.metadata[\"SamplingFrequency\"]\n",
    ")\n",
    "\n",
    "data_first_fixation = et_dwi.recording[\n",
    "    (et_dwi.recording[\"timestamp\"] >= start_first_fixation)\n",
    "    & (et_dwi.recording[\"timestamp\"] <= stop_first_fixation)\n",
    "]\n",
    "\n",
    "data_second_fixation = et_dwi.recording[\n",
    "    (et_dwi.recording[\"timestamp\"] >= start_second_fixation)\n",
    "    & (et_dwi.recording[\"timestamp\"] <= stop_second_fixation)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3127b6",
   "metadata": {},
   "source": [
    "We can now create an animation that displays the heatmap of the coordinates during those two fixation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7116bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    if frame == 0:\n",
    "        return plot.plot_heatmap_coordinate(\n",
    "            data_first_fixation, ax=ax, density=False, screen_size=size\n",
    "        )\n",
    "    else:\n",
    "        return plot.plot_heatmap_coordinate(\n",
    "            data_second_fixation, ax=ax, density=False, screen_size=size\n",
    "        )\n",
    "\n",
    "\n",
    "num_frames = 2\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, update, frames=num_frames, interval=2000, repeat=True, blit=False\n",
    ")\n",
    "\n",
    "plt.close('all')\n",
    "plt.clf()\n",
    "HTML(anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4d1ed",
   "metadata": {},
   "source": [
    "It's interesting to observe that the gaze during the second fixation wasn't centered on the screen, which is unexpected. This happens because the participant's eye was mostly closed during that period. To verify this, let's plot the blink data for the last fixation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baeb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    (data_second_fixation[\"timestamp\"].values - et_dwi.metadata[\"StartTime\"])\n",
    "    / et_dwi.metadata[\"SamplingFrequency\"],\n",
    "    data_second_fixation[\"eye1_blink\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"eyes closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348794d",
   "metadata": {},
   "source": [
    "# Quality Control task (qct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc34d0",
   "metadata": {},
   "source": [
    "Now, let's repeat the process for the quality control task. We'll encapsulate the session data and showcase the same visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928945ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(et)\n",
    "et_qct = et.EyeTrackingRun.from_edf(\n",
    "    DATA_PATH / et_session.qct_edf.values[0],\n",
    "    message_first_trigger='hello qct',\n",
    "    message_last_trigger='bye qct',\n",
    ")\n",
    "t_axis = (et_qct.recording.timestamp.values - et_qct.metadata[\"StartTime\"]) / et_qct.metadata[\"SamplingFrequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad543d44",
   "metadata": {},
   "source": [
    "First, let's look at the pupil size after filtering out blinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2));\n",
    "plt.plot(\n",
    "    t_axis[et_qct.recording.eye1_blink == 0],\n",
    "    et_qct.recording.loc[et_qct.recording.eye1_blink == 0, \"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\");\n",
    "plt.ylabel(\"pupil area [a.u.]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88deae5",
   "metadata": {},
   "source": [
    "The duration of eyeblinks provides insights into the participant's wakefulness during the task. Let's visualize the eyeblink duration specifically for the QCT. Here the plot looks very different compared to the DWI run. The short blink durations in the QCT plot suggest that the participant likely kept their tracked eye open throughout the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f476ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_qct.recording[\"eye1_blink\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"eyes closed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94367b",
   "metadata": {},
   "source": [
    "The QCT comprises four randomly ordered tasks: visual gratings, fingertapping with hand instructions, cognitive gaze movement, and blank trials with a central fixation point. Messages are sent by the Psychopy experiment at the beginning and end of each subtask, and these are stored in the 'LoggedMessages' field of et_qct.metadata. Let's examine these messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_messages=et_qct.metadata['LoggedMessages']\n",
    "logged_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6325c",
   "metadata": {},
   "source": [
    "To visualize the data of each subtask separately, let's construct a dictionary. Each entry will encapsulate a dataframe with data from one of these subtasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419725d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask_dataframes = {}\n",
    "n_task = 0\n",
    "for i in range(len(logged_messages)):\n",
    "    entry = logged_messages[i]\n",
    "    if \"start\" in entry[1]:\n",
    "        start_time = entry[0]\n",
    "        subtask_type = entry[1].split()[1]\n",
    "        stop_time = None\n",
    "\n",
    "        for j in range(i + 1, len(logged_messages)):\n",
    "\n",
    "            stop_entry = logged_messages[j]\n",
    "            if \"stop\" in stop_entry[1] and (subtask_type in stop_entry[1]):\n",
    "                stop_time = stop_entry[0]\n",
    "                n_task = n_task + 1\n",
    "                break\n",
    "\n",
    "        if stop_time is not None:\n",
    "\n",
    "            subtask_data = et_qct.recording[\n",
    "                (et_qct.recording[\"timestamp\"] >= start_time)\n",
    "                & (et_qct.recording[\"timestamp\"] <= stop_time)\n",
    "            ]\n",
    "\n",
    "            df_name = f\"df_{subtask_type}_{n_task}\"\n",
    "            subtask_dataframes[df_name] = subtask_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbbcd0",
   "metadata": {},
   "source": [
    "Now we can plot the heatmap of the gaze coordinates for each of the subtasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa32763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "size = (800, 600)\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    dataframe = list(subtask_dataframes.values())[frame]\n",
    "\n",
    "    if dataframe is not None:\n",
    "        plot.plot_heatmap_coordinate(\n",
    "            dataframe[[\"eye1_x_coordinate\", \"eye1_y_coordinate\"]],\n",
    "            ax=ax,\n",
    "            density=False,\n",
    "            screen_size=size,\n",
    "        )\n",
    "\n",
    "        subtask_name = (\n",
    "            list(subtask_dataframes.keys())[frame].split(\"_\")[1].capitalize()\n",
    "        )\n",
    "        ax.set_title(f\"Subtask: {subtask_name}\")\n",
    "\n",
    "\n",
    "num_frames = len(subtask_dataframes)\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, update, frames=num_frames, interval=1000, repeat=True, blit=False\n",
    ")\n",
    "plt.close(\"all\")\n",
    "plt.clf()\n",
    "HTML(anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b0893",
   "metadata": {},
   "source": [
    "During cognitive (cog) events, a noticeable bimodal distribution in gaze coordinates emerges, reflecting the participant's delayed reaction to stimuli. The initial portion of the gaze distribution persists on the coordinates of the preceding stimuli, while the subsequent part aligns with the coordinates of the currently displayed stimulus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f2f70",
   "metadata": {},
   "source": [
    "# rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e1667e",
   "metadata": {},
   "source": [
    "Now, let's do the same for the resting state. We'll encapsulate the session data and show similar visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4414bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(et)\n",
    "et_rest = et.EyeTrackingRun.from_edf(\n",
    "    DATA_PATH / et_session.rest_edf.values[0],\n",
    "    message_first_trigger=\"start movie\",\n",
    "    message_last_trigger=\"Bye rs\",\n",
    ")\n",
    "t_axis = (\n",
    "    et_rest.recording.timestamp.values - et_rest.metadata[\"StartTime\"]\n",
    ") / et_rest.metadata[\"SamplingFrequency\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63c7d0",
   "metadata": {},
   "source": [
    "Let's first have a look at the blinks duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_rest.recording[\"eye1_blink\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"eyes closed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00cb82",
   "metadata": {},
   "source": [
    "The resting state task comprises three events: initial and final fixation points for drift estimation and a 20-minute movie. Let's start by analyzing the data recorded during the movie. At the beginning and end of the movie, the Psychopy experiment sends the messages'start movie' and 'end movie'. The start of the movie correspond to the first trigger sent by the scanner. First, let's extract the data corresponding to the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_movie_timestamp = et_rest.metadata[\"StartTime\"]\n",
    "stop_movie_timestamp = [\n",
    "    entry[0] for entry in et_rest.metadata[\"LoggedMessages\"] if \"end movie\" in entry[1]\n",
    "][0]\n",
    "data_movie = et_rest.recording[\n",
    "    (et_rest.recording[\"timestamp\"] >= start_movie_timestamp)\n",
    "    & (et_rest.recording[\"timestamp\"] <= stop_movie_timestamp)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943d965",
   "metadata": {},
   "source": [
    "We can now plot the heatmap of the gaze coordinates during the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_heatmap_coordinate(\n",
    "    data_movie[[\"eye1_x_coordinate\", \"eye1_y_coordinate\"]],\n",
    "    density=False,\n",
    "    screen_size=size,\n",
    "    background_image=\"mundaka-image.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1b3ad",
   "metadata": {},
   "source": [
    "This provides a general overview of the most focal area for the gaze. Now, for a dynamic view of how the gaze moved during the movie, we can run the next cell. It will generate an animated plot illustrating the trajectory of the gaze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79401444",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "def update(frame, timestamps_per_frame, background_image):\n",
    "    start_idx = frame * timestamps_per_frame\n",
    "    end_idx = (frame + 1) * timestamps_per_frame\n",
    "    if end_idx > len(data_movie):\n",
    "        end_idx = len(data_movie)\n",
    "    subset_data = data_movie.iloc[start_idx:end_idx]\n",
    "    time = int(\n",
    "        (data_movie.iloc[start_idx][\"timestamp\"] - et_rest.metadata[\"StartTime\"])\n",
    "        / et_rest.metadata[\"SamplingFrequency\"]\n",
    "    )\n",
    "    avg_x = subset_data[\"eye1_x_coordinate\"].mean()\n",
    "    avg_y = subset_data[\"eye1_y_coordinate\"].mean()\n",
    "\n",
    "    ax.clear()\n",
    "    ax.scatter(avg_x, avg_y, color=\"red\", marker=\"o\", label=\"Average Gaze Position\")\n",
    "    ax.legend()\n",
    "\n",
    "    extent = [\n",
    "        et_rest.metadata[\"ScreenAOIDefinition\"][1][0],\n",
    "        et_rest.metadata[\"ScreenAOIDefinition\"][1][1],\n",
    "        et_rest.metadata[\"ScreenAOIDefinition\"][1][3],\n",
    "        et_rest.metadata[\"ScreenAOIDefinition\"][1][2],\n",
    "    ]\n",
    "    background = mpimg.imread(background_image)\n",
    "    ax.imshow(background, zorder=0, extent=extent, alpha=0.7)\n",
    "    ax.set_title(f\"Time {time} s\")\n",
    "    ax.set_xlim(extent[0], extent[1])\n",
    "    ax.set_ylim(extent[3], extent[2])\n",
    "    ax.invert_yaxis()\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "timestamps_per_frame = 4000\n",
    "background_image = \"mundaka-image.png\"\n",
    "anim = animation.FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=len(data_movie) // timestamps_per_frame,\n",
    "    repeat=False,\n",
    "    blit=False,\n",
    "    fargs=(timestamps_per_frame, background_image),\n",
    ")\n",
    "HTML(anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df94a29",
   "metadata": {},
   "source": [
    "Now, let's use the two fixation points to get an idea of the drift during the resting state run, similar to the approach used for the DWI run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae672005",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_first_fixation = [\n",
    "    entry[0]\n",
    "    for entry in et_rest.metadata[\"LoggedMessages\"]\n",
    "    if \"start fixation\" in entry[1]\n",
    "][0]\n",
    "stop_first_fixation = [\n",
    "    entry[0]\n",
    "    for entry in et_rest.metadata[\"LoggedMessages\"]\n",
    "    if \"end fixation\" in entry[1]\n",
    "][0]\n",
    "stop_second_fixation = [\n",
    "    entry[0]\n",
    "    for entry in et_rest.metadata[\"LoggedMessages\"]\n",
    "    if \"end fixation\" in entry[1]\n",
    "][1]\n",
    "start_second_fixation = [\n",
    "    entry[0]\n",
    "    for entry in et_rest.metadata[\"LoggedMessages\"]\n",
    "    if \"start fixation\" in entry[1]\n",
    "][1]\n",
    "\n",
    "data_first_fixation = et_rest.recording[\n",
    "    (et_rest.recording[\"timestamp\"] >= start_first_fixation)\n",
    "    & (et_rest.recording[\"timestamp\"] <= stop_first_fixation)\n",
    "]\n",
    "data_second_fixation = et_rest.recording[\n",
    "    (et_rest.recording[\"timestamp\"] >= start_second_fixation)\n",
    "    & (et_rest.recording[\"timestamp\"] <= stop_second_fixation)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    if frame == 0:\n",
    "        return plot.plot_heatmap_coordinate(\n",
    "            data_first_fixation, ax=ax, density=False, screen_size=size\n",
    "        )\n",
    "    else:\n",
    "        return plot.plot_heatmap_coordinate(\n",
    "            data_second_fixation, ax=ax, density=False, screen_size=size\n",
    "        )\n",
    "\n",
    "\n",
    "num_frames = 2\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, update, frames=num_frames, interval=2000, repeat=False, blit=False\n",
    ")\n",
    "plt.close(\"all\")\n",
    "plt.clf()\n",
    "\n",
    "HTML(anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94165c",
   "metadata": {},
   "source": [
    "# bht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5034f48",
   "metadata": {},
   "source": [
    "Lastly, let's explore the breath-holding task data. To start, we'll look at the changes in pupil size throughout the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(et)\n",
    "et_bht = et.EyeTrackingRun.from_edf(\n",
    "    DATA_PATH / et_session.bht_edf.values[0],\n",
    "    message_first_trigger=\"hello bht\",\n",
    "    message_last_trigger=\"Bye bht\",\n",
    ")\n",
    "t_axis = (\n",
    "    et_bht.recording.timestamp.values - et_bht.metadata[\"StartTime\"]\n",
    ") / et_bht.metadata[\"SamplingFrequency\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f99b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt.clf()\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(16, 2));\n",
    "plt.plot(\n",
    "    t_axis[et_bht.recording.eye1_blink == 0],\n",
    "    et_bht.recording.loc[et_bht.recording.eye1_blink == 0, \"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\");\n",
    "plt.ylabel(\"pupil area [a.u.]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e794323",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_extract = [\n",
    "    \"start mock block\",\n",
    "    \"stop mock block\",\n",
    "    \"start bh block\",\n",
    "    \"stop bh block\",\n",
    "]\n",
    "\n",
    "timestamps = {event.replace(\" \", \"_\"): None for event in events_to_extract}\n",
    "\n",
    "for timestamp, event in et_bht.metadata[\"LoggedMessages\"]:\n",
    "    if event in events_to_extract:\n",
    "        timestamps[event.replace(\" \", \"_\")] = int(timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74033150",
   "metadata": {},
   "source": [
    "The breath-holding task included a mock block where participants didn't follow stimuli and five real blocks where they were instructed to breathe in, out, and hold based on the displayed rectangle colors. In the real blocks, participants had a 10-second break after each breath hold, with the screen turning entirely black. The pupil size plot shows an increase during these breaks, notably more pronounced in the second, third, and fourth blocks. To make this clearer, let's mark the break timings directly on the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbae44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = et_bht.metadata[\"SamplingFrequency\"]\n",
    "timestamps_in_seconds = {\n",
    "    event: (timestamp - et_bht.metadata[\"StartTime\"]) / sampling_frequency\n",
    "    for event, timestamp in timestamps.items()\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.plot(\n",
    "    t_axis[et_bht.recording.eye1_blink == 0],\n",
    "    et_bht.recording.loc[et_bht.recording.eye1_blink == 0, \"eye1_pupil_size\"].values,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"pupil area [a.u.]\")\n",
    "\n",
    "\n",
    "for event, timestamp in timestamps_in_seconds.items():\n",
    "    plt.axvline(\n",
    "        x=timestamp, color=\"red\", linestyle=\"--\", label=f\"{event} ({timestamp:.2f} s)\"\n",
    "    )\n",
    "plt.axvline(\n",
    "    x=timestamps_in_seconds[\"start_bh_block\"] + 40,\n",
    "    color=\"violet\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"break\",\n",
    ")\n",
    "for n in np.arange(1, 5):\n",
    "    plt.axvline(\n",
    "        x=timestamps_in_seconds[\"start_bh_block\"] + 40 * (n + 1) + 10 * n,\n",
    "        color=\"violet\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262ea8e",
   "metadata": {},
   "source": [
    "Another interesting observation is a sudden dip in pupil size around 67 seconds from the beginning. This dip coincides with a point where the eye tracking failed, causing a divergence in the y-coordinate. Let's explore this further by visualizing the pupil size and y-coordinate simultaneously around this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "ax1.plot(\n",
    "    t_axis[et_bht.recording.eye1_blink == 0],\n",
    "    et_bht.recording.loc[et_bht.recording.eye1_blink == 0, \"eye1_pupil_size\"].values,\n",
    "    'blue',\n",
    "    label=\"Pupil Size\"\n",
    ")\n",
    "ax1.set_xlabel(\"time [s]\")\n",
    "ax1.set_ylabel(\"Pupil Size [a.u.]\", color='blue')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    t_axis,\n",
    "    et_bht.recording[\"eye1_y_coordinate\"].values,\n",
    "    'orange', \n",
    "    label=\"Y gaze coordinate\"\n",
    ")\n",
    "ax2.set_ylabel(\"Y Coordinate\", color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "\n",
    "plt.xlim(67, 68.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc4e9a",
   "metadata": {},
   "source": [
    "Now, mirroring our approach for the previous tasks, let's closely examine the eyeblink duration throughout this session to ensure the participant stayed alert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 2))\n",
    "plt.plot(\n",
    "    t_axis,\n",
    "    et_bht.recording[\"eye1_blink\"].values,\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"eyes closed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d5ea5",
   "metadata": {},
   "source": [
    "Finally, let's check out the gaze coordinates density map during the task. Since the rectangles were centered on the screen, we expect the gaze to be mostly concentrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_heatmap_coordinate(\n",
    "    et_bht.recording[[\"eye1_x_coordinate\", \"eye1_y_coordinate\"]],\n",
    "    density=False,\n",
    "    screen_size=size,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyedf310",
   "language": "python",
   "name": "pyedf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
